# MIT License
#
# Copyright (c) 2020 Brockmann Consult GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import json
import os
import pathlib
import re
from typing import List, Optional
import xarray as xr
import xcube
from xcube.core.store import DataDescriptor, DatasetDescriptor, \
    VariableDescriptor
from xcube.util.jsonschema import JsonObjectSchema, JsonStringSchema, \
    JsonArraySchema, JsonNumberSchema

from xcube_cds.store import CDSDatasetHandler


class ERA5DatasetHandler(CDSDatasetHandler):

    def __init__(self):
        self._read_dataset_info()

    def _read_dataset_info(self):
        """Read dataset information from JSON files"""

        # Information for each supported dataset is contained in a
        # semi-automatically generated JSON file. The largest part of this
        # file is the "variables" table. This table maps request parameter
        # names to NetCDF variable names, and was generated by the following
        # process:
        #
        # 1. Obtain the complete list of valid request parameters via the Web
        #    interface by selecting every box and copying the parameter names
        #    out of the generated API request.
        #
        # 2. For each request parameter, make a separate API request
        #    containing only that request parameter, producing a NetCDF file
        #    containing only the corresponding output parameter.
        #
        # 3. Read the name of the single output variable from the NetCDF file
        #    and collate it with the original request parameter. (Also read the
        #    long_name and units attributes.)
        #
        # In this way we are guaranteed to get the correct NetCDF variable
        # name for each request parameter, without having to trust that the
        # documentation is correct.
        #
        # Unfortunately this procedure doesn't work with all datasets, since
        # some (e.g. satellite-soil-moisture) don't have a one-to-one mapping
        # from request variables to output variables.
        #
        # Table fields are:
        # 1. request parameter name in CDS API
        # 2. NetCDF variable name (NB: not always CF-conformant)
        # 3. units from NetCDF attributes
        # 4. "long name" from NetCDF attributes

        ds_info_path = pathlib.Path(__file__).parent
        all_pathnames = [os.path.join(ds_info_path, leafname)
                         for leafname in os.listdir(ds_info_path)]
        pathnames = filter(lambda p: os.path.isfile(p) and p.endswith('.json'),
                           all_pathnames)
        self._dataset_dicts = {}
        for pathname in pathnames:
            with open(pathname, 'r') as fh:
                ds_dict = json.load(fh)
                _, leafname = os.path.split(pathname)
                self._dataset_dicts[leafname[:-5]] = ds_dict

        self._valid_data_ids = set()
        self._data_id_to_human_readable = {}
        for ds_id, ds_dict in self._dataset_dicts.items():
            # product_type is actually a request parameter, but we implement
            # it as a suffix to the data_id to make it possible to specify
            # requests using only the standard, known store parameters.
            product_types = ds_dict['product_types']
            if len(product_types) == 0:
                # No product types defined (i.e. there is just a single,
                # implicit product type), so we just use the dataset ID without
                # a suffix.
                self._valid_data_ids.add(ds_id)
                self._data_id_to_human_readable[ds_id] = ds_dict['description']
            else:
                for pt_id, pt_desc in product_types:
                    data_id = ds_id + ':' + pt_id
                    self._valid_data_ids.add(data_id)
                    self._data_id_to_human_readable[data_id] = \
                        ds_dict['description'] + ' \N{EN DASH} ' + pt_desc

    def get_supported_data_ids(self) -> List[str]:
        return list(self._valid_data_ids)

    def get_open_data_params_schema(self, data_id: Optional[str] = None) -> \
            JsonObjectSchema:
        # If the data_id has a product type suffix, remove it.
        dataset_id = data_id.split(':')[0] if ':' in data_id else data_id

        ds_info = self._dataset_dicts[dataset_id]
        variable_info_table = ds_info['variables']
        bbox = ds_info['bbox']

        params = dict(
            dataset_name=JsonStringSchema(min_length=1,
                                          enum=list(self._valid_data_ids)),
            variable_names=JsonArraySchema(
                items=(JsonStringSchema(
                    min_length=1,
                    enum=[cds_api_name
                          for cds_api_name, _, _, _ in variable_info_table]
                )),
                unique_items=True
            ),
            crs=JsonStringSchema(nullable=True, default=ds_info['crs'],
                                 enum=[None, ds_info['crs']]),
            # W, S, E, N (will be converted to N, W, S, E)
            bbox=JsonArraySchema(items=(
                JsonNumberSchema(minimum=bbox[1], maximum=bbox[3]),
                JsonNumberSchema(minimum=bbox[2], maximum=bbox[0]),
                JsonNumberSchema(minimum=bbox[1], maximum=bbox[3]),
                JsonNumberSchema(minimum=bbox[2], maximum=bbox[0]))),
            spatial_res=JsonNumberSchema(minimum=ds_info['spatial_res'],
                                         maximum=10,
                                         default=ds_info['spatial_res']),
            time_range=JsonArraySchema(
                items=[JsonStringSchema(format='date-time'),
                       JsonStringSchema(format='date-time', nullable=True)]),
            time_period=JsonStringSchema(const=ds_info['time_period']),
        )
        required = [
            'variable_names',
            'bbox',
            'spatial_res',
            'time_range',
        ]
        return JsonObjectSchema(
            properties=dict(
                **params,
            ),
            required=required
        )

    def get_human_readable_data_id(self, data_id: str):
        return self._data_id_to_human_readable[data_id]

    def describe_data(self, data_id: str) -> DataDescriptor:
        ds_info = self._dataset_dicts[data_id]

        return DatasetDescriptor(
            data_id=data_id,
            data_vars=self._create_variable_descriptors(data_id),
            crs=ds_info['crs'],
            bbox=tuple(ds_info['bbox']),
            spatial_res=ds_info['spatial_res'],
            time_range=tuple(ds_info['time_range']),
            time_period=ds_info('time_period'),
            open_params_schema=self.get_open_data_params_schema(data_id)
        )

    def _create_variable_descriptors(self, data_id):
        dataset_id, _ = data_id.split(':')

        return [
            VariableDescriptor(
                name=netcdf_name,
                # dtype string format not formally defined as of 2020-06-18.
                # t2m is actually stored as a short with scale and offset in
                # the NetCDF file, but converted to float by xarray on opening:
                # see http://xarray.pydata.org/en/stable/io.html .
                dtype='float32',
                dims=('time', 'latitude', 'longitude'),
                attrs=dict(units=units, long_name=long_name))
            for (api_name, netcdf_name, units, long_name)
            in self._dataset_dicts[dataset_id]['variables']
        ]

    def transform_params(self, plugin_params, data_id):
        """Transform supplied parameters to CDS API format.

        :param plugin_params: parameters in form expected by this plugin
        :param data_id: the ID of the requested dataset
        :return: parameters in form expected by the CDS API
        """

        dataset_name, product_type = \
            data_id.split(':') if ':' in data_id else (data_id, None)

        # We need to split out the bounding box co-ordinates to re-order them.
        x1, y1, x2, y2 = plugin_params['bbox']

        # Translate our parameters (excluding time parameters) to the CDS API
        # scheme.
        resolution = plugin_params['spatial_res']
        params_combined = {
            'variable': plugin_params['variable_names'],
            # For the ERA5 dataset, we need to crop the area by half a
            # cell-width. ERA5 data are points, but xcube treats them as
            # cell centres. The bounds of a grid of cells are half a cell-width
            # outside the bounds of a grid of points, so we have to crop each
            # edge by half a cell-width to end up with the requested bounds.
            # See https://confluence.ecmwf.int/display/CKB/ERA5%3A+What+is+the+spatial+reference#ERA5:Whatisthespatialreference-Visualisationofregularlat/londata
            'area': [y2 - resolution / 2,
                     x1 + resolution / 2,
                     y1 + resolution / 2,
                     x2 - resolution / 2],
            # Note: the "grid" parameter is not exposed via the web interface,
            # but is described at
            # https://confluence.ecmwf.int/display/CKB/ERA5%3A+Web+API+to+CDS+API
            'grid': [resolution, resolution],
            'format': 'netcdf'
        }

        if product_type is not None:
            params_combined['product_type'] = product_type

        # Convert the time range specification to the nearest equivalent
        # in the CDS "orthogonal time units" scheme.
        time_params_from_range = self.transform_time_params(
            self.convert_time_range(plugin_params['time_range']))
        params_combined.update(time_params_from_range)

        # If any of the "years", "months", "days", and "hours" parameters
        # were passed, they override the time specifications above.
        time_params_explicit = \
            self.transform_time_params(plugin_params)
        params_combined.update(time_params_explicit)

        # Transform singleton list values into their single members, as
        # required by the CDS API.
        desingletonned = {
            k: (v[0] if isinstance(v, list) and len(v) == 1 else v)
            for k, v in params_combined.items()}

        return dataset_name, desingletonned

    def read_file(self, dataset_name, cds_api_params, file_path):

        # decode_cf is the default, but it's clearer to make it explicit.
        return xr.open_dataset(file_path, decode_cf=True)

