# MIT License
#
# Copyright (c) 2020 Brockmann Consult GmbH
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import atexit
import datetime
import dateutil.parser
import dateutil.rrule
import dateutil.relativedelta
import json
import os
import pathlib
import re
import shutil
import tempfile
from typing import Iterator, Tuple, List, Optional, Dict

import cdsapi
import xarray as xr

from xcube.core.store import DataDescriptor
from xcube.core.store import DataOpener
from xcube.core.store import DataStore
from xcube.core.store import DataStoreError
from xcube.core.store import DatasetDescriptor
from xcube.core.store import TYPE_ID_DATASET
from xcube.core.store import VariableDescriptor
from xcube.util.jsonschema import JsonArraySchema
from xcube.util.jsonschema import JsonBooleanSchema
from xcube.util.jsonschema import JsonIntegerSchema
from xcube.util.jsonschema import JsonNumberSchema
from xcube.util.jsonschema import JsonObjectSchema
from xcube.util.jsonschema import JsonStringSchema
from xcube_cds.constants import CDS_DATA_OPENER_ID
from xcube_cds.constants import DEFAULT_NUM_RETRIES
import xcube.core.normalize


class CDSDataOpener(DataOpener):
    """A data opener for the Copernicus Climate Data Store"""

    def __init__(self, normalize_names: Optional[bool] = False):
        self._normalize_names = normalize_names
        self._create_temporary_directory()
        self._read_dataset_info()

    def _create_temporary_directory(self):
        # Create a temporary directory to hold downloaded NetCDF files and
        # a hook to delete it when the interpreter exits. xarray.open reads
        # data lazily so we can't just delete the file after returning the
        # Dataset. We could also use weakref hooks to delete individual files
        # when the corresponding object is garbage collected, but even then
        # the directory is useful to group the files and offer an extra
        # assurance that they will be deleted.
        tempdir = tempfile.mkdtemp()

        def delete_tempdir():
            shutil.rmtree(tempdir, ignore_errors=True)

        atexit.register(delete_tempdir)
        self._tempdir = tempdir

    def _read_dataset_info(self):
        """Read dataset information from JSON files"""

        # Information for each supported dataset is contained in a
        # semi-automatically generated JSON file. The largest part of this
        # file is the "variables" table. This table maps request parameter
        # names to NetCDF variable names, and was generated by the following
        # process:
        #
        # 1. Obtain the complete list of valid request parameters via the Web
        #    interface by selecting every box and copying the parameter names
        #    out of the generated API request.
        #
        # 2. For each request parameter, make a separate API request
        #    containing only that request parameter, producing a NetCDF file
        #    containing only the corresponding output parameter.
        #
        # 3. Read the name of the single output variable from the NetCDF file
        #    and collate it with the original request parameter. (Also read the
        #    long_name and units attributes.)
        #
        # In this way we are guaranteed to get the correct NetCDF variable
        # name for each request parameter, without having to trust that the
        # documentation is correct.
        #
        # Table fields are:
        # 1. request parameter name in CDS API
        # 2. NetCDF variable name (NB: not always CF-conformant)
        # 3. units from NetCDF attributes
        # 4. "long name" from NetCDF attributes

        ds_info_path = pathlib.Path(__file__).parent / 'datasets'
        all_pathnames = [os.path.join(ds_info_path, leafname)
                         for leafname in os.listdir(ds_info_path)]
        pathnames = filter(lambda p: os.path.isfile(p) and p.endswith('.json'),
                           all_pathnames)
        self._dataset_dicts = {}
        for pathname in pathnames:
            with open(pathname, 'r') as fh:
                ds_dict = json.load(fh)
                _, leafname = os.path.split(pathname)
                self._dataset_dicts[leafname[:-5]] = ds_dict

        self._valid_data_ids = set()
        self._data_id_to_human_readable = {}
        for ds_id, ds_dict in self._dataset_dicts.items():
            # product_type is actually a request parameter, but we implement
            # it as a suffix to the data_id to make it possible to specify
            # requests using only the standard, known store parameters.
            product_types = ds_dict['product_types']
            if len(product_types) == 0:
                # No product types defined (i.e. there is just a single,
                # implicit product type), so we just use the dataset ID without
                # a suffix.
                self._valid_data_ids.add(ds_id)
                self._data_id_to_human_readable[ds_id] = ds_dict['description']
            else:
                for pt_id, pt_desc in product_types:
                    data_id = ds_id + ':' + pt_id
                    self._valid_data_ids.add(data_id)
                    self._data_id_to_human_readable[data_id] = \
                        ds_dict['description'] + ' \N{EN DASH} ' + pt_desc


    ###########################################################################
    # DataOpener implementation

    def get_open_data_params_schema(self, data_id: Optional[str] = None) -> \
            JsonObjectSchema:

        self._validate_data_id(data_id, allow_none=True)

        # TODO: define a broad default schema here for the case where
        # data_id==None. If data_id is supplied, its values can be
        # overwritten.

        # If the data_id has a product type suffix, remove it.
        dataset_id = data_id.split(':')[0] if ':' in data_id else data_id

        ds_info = self._dataset_dicts[dataset_id]
        variable_info_table = ds_info['variables']
        bbox = ds_info['bbox']

        params = dict(
            dataset_name=JsonStringSchema(min_length=1,
                                          enum=list(self._valid_data_ids)),
            variable_names=JsonArraySchema(
                items=(JsonStringSchema(
                    min_length=1,
                    enum=[cds_api_name
                          for cds_api_name, _, _, _ in variable_info_table]
                )),
                unique_items=True
            ),
            crs=JsonStringSchema(nullable=True, default=ds_info['crs'],
                                 enum=[None, ds_info['crs']]),
            # W, S, E, N (will be converted to N, W, S, E)
            bbox=JsonArraySchema(items=(
                JsonNumberSchema(minimum=bbox[1], maximum=bbox[3]),
                JsonNumberSchema(minimum=bbox[2], maximum=bbox[0]),
                JsonNumberSchema(minimum=bbox[1], maximum=bbox[3]),
                JsonNumberSchema(minimum=bbox[2], maximum=bbox[0]))),
            spatial_res=JsonNumberSchema(minimum=ds_info['spatial_res'],
                                         maximum=10,
                                         default=ds_info['spatial_res']),
            time_range=JsonArraySchema(
                items=[JsonStringSchema(format='date-time'),
                       JsonStringSchema(format='date-time', nullable=True)]),
            time_period=JsonStringSchema(const=ds_info['time_period']),
        )

        # TODO: work out what to do with these ERA5-specific parameters.
        # Is it worth keeping them if the UI won't support anything beyond
        # the standard parameters anyway?
        #
        # hours = JsonArraySchema(
        #     items=JsonIntegerSchema(minimum=0, maximum=23),
        #     unique_items=True,
        #     min_items=1
        # ),
        # months = JsonArraySchema(
        #     items=JsonIntegerSchema(minimum=1, maximum=12),
        #     unique_items=True,
        #     min_items=1
        # ),
        # years = JsonArraySchema(
        #     items=JsonIntegerSchema(minimum=1979, maximum=2020),
        #     unique_items=True,
        #     min_items=1
        # ),

        required = [
            'variable_names',
            'bbox',
            'spatial_res',
            'time_range',
        ]
        return JsonObjectSchema(
            properties=dict(
                **params,
            ),
            required=required
        )

    def open_data(self, data_id: str, **open_params) -> xr.Dataset:
        schema = self.get_open_data_params_schema(data_id)
        schema.validate_instance(open_params)

        client = cdsapi.Client()

        # We can't generate a safe unique filename (since the file is created
        # by client.retrieve, so name generation and file creation won't be
        # atomic). Instead we atomically create a subdirectory of the temporary
        # directory for the single file.
        subdir = tempfile.mkdtemp(dir=self._tempdir)
        file_path = os.path.join(subdir, 'data.nc')
        dataset_name, cds_api_params = \
            CDSDataOpener._transform_params(open_params, data_id)

        # This call returns a Result object, which at present we make
        # no use of.
        client.retrieve(dataset_name, cds_api_params, file_path)

        # decode_cf is the default, but it's clearer to make it explicit.
        dataset = xr.open_dataset(file_path, decode_cf=True)

        # The API doesn't close the session automatically, so we need to
        # do it explicitly here to avoid leaving an open socket.
        client.session.close()

        dataset = dataset.rename_dims({
            'longitude': 'lon',
            'latitude': 'lat'
        })
        dataset = dataset.rename_vars({'longitude': 'lon', 'latitude': 'lat'})
        dataset.transpose('time', ..., 'lat', 'lon')
        dataset.coords['time'].attrs['standard_name'] = 'time'
        dataset.coords['lat'].attrs['standard_name'] = 'latitude'
        dataset.coords['lon'].attrs['standard_name'] = 'longitude'

        # Correct units not entirely clear: cubespec document says
        # degrees_north / degrees_east for WGS84 Schema, but SH Plugin
        # had decimal_degrees.
        dataset.coords['lat'].attrs['units'] = 'degrees_north'
        dataset.coords['lon'].attrs['units'] = 'degrees_east'

        # TODO: Temporal coordinate variables MUST have units, standard_name,
        # and any others. standard_name MUST be "time", units MUST have
        # format "<deltatime> since <datetime>", where datetime must have
        # ISO-format.

        dataset = xcube.core.normalize.normalize_dataset(dataset)

        if self._normalize_names:
            rename_dict = {}
            for name in dataset.data_vars.keys():
                normalized_name = re.sub(r'\W|^(?=\d)', '_', name)
                if name != normalized_name:
                    rename_dict[name] = normalized_name
            dataset_renamed = dataset.rename_vars(rename_dict)
            return dataset_renamed
        else:
            return dataset

    @staticmethod
    def _transform_params(plugin_params, data_id):
        """Transform supplied parameters to CDS API format.

        :param plugin_params: parameters in form expected by this plugin
        :return: parameters in form expected by the CDS API
        """

        dataset_name, product_type = \
            data_id.split(':') if ':' in data_id else data_id, None

        # We need to split out the bounding box co-ordinates to re-order them.
        x1, y1, x2, y2 = plugin_params['bbox']

        # Translate our parameters (excluding time parameters) to the CDS API
        # scheme.
        params_combined = {
            'variable': plugin_params['variable_names'],
            'area': [y2, x1, y1, x2],
            # Note: the "grid" parameter is not exposed via the web interface,
            # but is described at
            # https://confluence.ecmwf.int/display/CKB/ERA5%3A+Web+API+to+CDS+API .
            'grid': [plugin_params['spatial_res'],
                     plugin_params['spatial_res']],
            'format': 'netcdf'
        }

        if product_type is not None:
            params_combined['product_type'] = product_type

        # Convert the time range specification to the nearest equivalent
        # in the CDS "orthogonal time units" scheme.
        time_params_from_range = CDSDataOpener._transform_time_params(
            CDSDataOpener._convert_time_range(plugin_params['time_range']))
        params_combined.update(time_params_from_range)

        # If any of the "years", "months", "days", and "hours" parameters
        # were passed, they override the time specifications above.
        time_params_explicit = \
            CDSDataOpener._transform_time_params(plugin_params)
        params_combined.update(time_params_explicit)

        # Transform singleton list values into their single members, as
        # required by the CDS API.
        desingletonned = {
            k: (v[0] if isinstance(v, list) and len(v) == 1 else v)
            for k, v in params_combined.items()}

        return dataset_name, desingletonned

    @staticmethod
    def _transform_time_params(params: Dict):
        return {
            k1: v1 for k1, v1 in [CDSDataOpener._transform_time_param(k0, v0)
                                  for k0, v0 in params.items()]
            if k1 is not None}

    @staticmethod
    def _transform_time_param(key, value):
        if key == 'hours':
            return 'time', list(map(lambda x: f'{x:02d}:00', value))
        if key == 'days':
            return 'day', list(map(lambda x: f'{x:02d}', value))
        elif key == 'months':
            return 'month', list(map(lambda x: f'{x:02d}', value))
        elif key == 'years':
            return 'year', list(map(lambda x: f'{x:04d}', value))
        else:
            return None, None

    @staticmethod
    def _convert_time_range(time_range: List[str]) -> \
            Dict[str, List[int]]:
        """Convert a time range to a CDS-style time specification.

        This method converts a time range specification (i.e. a straightforward
        pair of "start time" and "end time") into the closest corresponding
        specification for CDS datasets such as ERA5 (which allow orthogonal
        selection of subsets of years, months, days, and hours). "Closest"
        here means the narrowest selection which will cover the entire
        requested time range, although it will often cover significantly more.
        For example, the range 2000-12-31 to 2002-01-02 would be translated
        to a request for all of 2000-2002, since every hour, day, and month
        must be selected.

        :param time_range: a length-2 list of ISO-8601 date/time strings
        :return: a dictionary with keys 'hours', 'days', 'months', and
            'years', and values which are lists of ints
        """

        if len(time_range) != 2:
            raise ValueError(f'time_range must have a length of 2, '
                             'not {len(time_range)}.')

        time0 = dateutil.parser.isoparse(time_range[0])
        time1 = datetime.datetime.now() if time_range[1] is None \
            else dateutil.parser.isoparse(time_range[1])

        # We use datetime's recurrence rule features to enumerate the
        # hour / day / month numbers which intersect with the selected time
        # range.

        hour0 = datetime.datetime(time0.year, time0.month, time0.day,
                                  time0.hour, 0)
        hour1 = datetime.datetime(time1.year, time1.month, time1.day,
                                  time1.hour, 59)
        hours = [dt.hour for dt in dateutil.rrule.rrule(
            freq=dateutil.rrule.HOURLY, count=24,
            dtstart=hour0, until=hour1)]
        hours.sort()

        day0 = datetime.datetime(time0.year, time0.month, time0.day, 0, 1)
        day1 = datetime.datetime(time1.year, time1.month, time1.day, 23, 59)
        days = [dt.day for dt in dateutil.rrule.rrule(
            freq=dateutil.rrule.DAILY, count=31, dtstart=day0, until=day1)]
        days = sorted(set(days))

        month0 = datetime.datetime(time0.year, time0.month, 1)
        month1 = datetime.datetime(time1.year, time1.month, 28)
        months = [dt.month for dt in dateutil.rrule.rrule(
            freq=dateutil.rrule.MONTHLY, count=12,
            dtstart=month0, until=month1)]
        months.sort()

        years = list(range(time0.year, time1.year + 1))

        return dict(hours=hours,
                    days=days,
                    months=months, years=years)

    def _validate_data_id(self, data_id, allow_none=False):
        if (data_id is None) and allow_none:
            return
        if data_id not in self._valid_data_ids:
            raise ValueError(f'Unknown data id "{data_id}"')


class CDSDataStore(CDSDataOpener, DataStore):

    def __init__(self,
                 num_retries: Optional[int] = DEFAULT_NUM_RETRIES,
                 **kwargs):
        super().__init__(**kwargs)
        self.num_retries = num_retries

    ###########################################################################
    # DataStore implementation

    @classmethod
    def get_data_store_params_schema(cls) -> JsonObjectSchema:
        params = dict(
            normalize_names=JsonBooleanSchema(default=False)
        )

        # For now, let CDS API use defaults or environment variables for
        # most parameters.
        cds_params = dict(
            num_retries=JsonIntegerSchema(default=DEFAULT_NUM_RETRIES,
                                          minimum=0),
        )

        params.update(cds_params)
        return JsonObjectSchema(
            properties=params,
            required=None,
            additional_properties=False
        )

    @classmethod
    def get_type_ids(cls) -> Tuple[str, ...]:
        return TYPE_ID_DATASET,

    def get_data_ids(self, type_id: Optional[str] = None) -> \
            Iterator[Tuple[str, Optional[str]]]:
        self._assert_valid_type_id(type_id)
        return iter((data_id, self._data_id_to_human_readable[data_id])
                    for data_id in self._valid_data_ids)

    def has_data(self, data_id: str) -> bool:
        return data_id in self._valid_data_ids

    def describe_data(self, data_id: str) -> DataDescriptor:
        self._validate_data_id(data_id)
        ds_info = self._dataset_dicts[data_id]

        return DatasetDescriptor(
            data_id=data_id,
            data_vars=self._create_variable_descriptors(data_id),
            crs=ds_info['crs'],
            bbox=tuple(ds_info['bbox']),
            spatial_res=ds_info['spatial_res'],
            time_range=tuple(ds_info['time_range']),
            time_period=ds_info('time_period'),
            open_params_schema=self.get_open_data_params_schema(data_id)
        )

    def _create_variable_descriptors(self, data_id):
        dataset_id, _ = data_id.split(':')

        return [
            VariableDescriptor(
                name=netcdf_name,
                # dtype string format not formally defined as of 2020-06-18.
                # t2m is actually stored as a short with scale and offset in
                # the NetCDF file, but converted to float by xarray on opening:
                # see http://xarray.pydata.org/en/stable/io.html .
                dtype='float32',
                dims=('time', 'latitude', 'longitude'),
                attrs=dict(units=units, long_name=long_name))
            for (api_name, netcdf_name, units, long_name)
            in self._dataset_dicts[dataset_id]['variables']
        ]

    # noinspection PyTypeChecker
    def search_data(self, type_id: Optional[str] = None, **search_params) -> \
            Iterator[DataDescriptor]:
        self._assert_valid_type_id(type_id)
        raise NotImplementedError()

    def get_data_opener_ids(self, data_id: Optional[str] = None,
                            type_id: Optional[str] = None) -> \
            Tuple[str, ...]:
        self._assert_valid_type_id(type_id)
        self._assert_valid_opener_id(data_id)
        return CDS_DATA_OPENER_ID,

    def get_open_data_params_schema(self, data_id: Optional[str] = None,
                                    opener_id: Optional[str] = None) -> \
            JsonObjectSchema:
        self._assert_valid_opener_id(opener_id)
        self._validate_data_id(data_id)
        return super().get_open_data_params_schema(data_id)

    def open_data(self, data_id: str, opener_id: Optional[str] = None,
                  **open_params) -> xr.Dataset:
        self._assert_valid_opener_id(opener_id)
        self._validate_data_id(data_id)
        return super().open_data(data_id, **open_params)

    ###########################################################################
    # Implementation helpers

    @staticmethod
    def _assert_valid_type_id(type_id):
        if type_id is not None and type_id != TYPE_ID_DATASET:
            raise DataStoreError(
                f'Data type identifier must be "{TYPE_ID_DATASET}", '
                f'but got "{type_id}"')

    @staticmethod
    def _assert_valid_opener_id(opener_id):
        if opener_id is not None and opener_id != CDS_DATA_OPENER_ID:
            raise DataStoreError(
                f'Data opener identifier must be "{CDS_DATA_OPENER_ID}"'
                f'but got "{opener_id}"')
